{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using GPT2 to generate text like Shakespeare.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0NE5wn7nfYtrch0s85UgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigfacecatxue/Language-generation/blob/main/Using_GPT2_to_generate_text_like_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWwiFJmFQDeF",
        "outputId": "2714a18e-5253-40c7-e8ce-be53ff6176ca"
      },
      "source": [
        "# download train set\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 05:24:22--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-03-07 05:24:22 (14.9 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyTpMbGgQHOn",
        "outputId": "e996808b-b072-426d-c496-3856fe4e64d0"
      },
      "source": [
        "# make a temporary directory to store fine-tuned model\n",
        "!mkdir output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘output’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jDkes5cpRq_",
        "outputId": "00e88aaa-b7a7-49a4-d93a-9bf31ceb71c5"
      },
      "source": [
        "# install datasets and transformers\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2DvqgECRXK2",
        "outputId": "c34d983b-cf40-487c-a989-259ed9217261"
      },
      "source": [
        "# download pretrained language model\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_clm.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 05:24:37--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17016 (17K) [text/plain]\n",
            "Saving to: ‘run_clm.py.2’\n",
            "\n",
            "run_clm.py.2        100%[===================>]  16.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-07 05:24:38 (48.7 MB/s) - ‘run_clm.py.2’ saved [17016/17016]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "recs41ROW-j-",
        "outputId": "25b562ef-435d-49da-b70d-16353fe56f9d"
      },
      "source": [
        "# fine tune distilgpt2 model. gpt2 model takes almost 6 hours to fine tune, distilgpt2 takes 3 hours, so I chose distilgpt2. Feel free to try gpt2 or gpt2-medium\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path distilgpt2 \\\n",
        "    --train_file /content/input.txt \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --do_train \\\n",
        "    --output_dir output"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-07 05:27:44.926379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "03/07/2021 05:27:48 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "03/07/2021 05:27:48 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Mar07_05-27-47_eb1c98354f4e, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=0)\n",
            "03/07/2021 05:27:48 - WARNING - datasets.builder -   Using custom data configuration default-8ba53bf964e9a623\n",
            "03/07/2021 05:27:48 - WARNING - datasets.builder -   Reusing dataset text (/root/.cache/huggingface/datasets/text/default-8ba53bf964e9a623/0.0.0/293ecb642f9fca45b44ad1f90c8445c54b9d80b95ab3fca3cfa5e1e3d85d4a57)\n",
            "[INFO|file_utils.py:1302] 2021-03-07 05:27:49,647 >> https://huggingface.co/distilgpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzj6n9op_\n",
            "Downloading: 100% 762/762 [00:00<00:00, 482kB/s]\n",
            "[INFO|file_utils.py:1306] 2021-03-07 05:27:50,198 >> storing https://huggingface.co/distilgpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|file_utils.py:1309] 2021-03-07 05:27:50,198 >> creating metadata file for /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:449] 2021-03-07 05:27:50,198 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:485] 2021-03-07 05:27:50,199 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:449] 2021-03-07 05:27:50,750 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:485] 2021-03-07 05:27:50,751 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1302] 2021-03-07 05:27:51,310 >> https://huggingface.co/distilgpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5zrfkuiu\n",
            "Downloading: 100% 1.04M/1.04M [00:01<00:00, 957kB/s]\n",
            "[INFO|file_utils.py:1306] 2021-03-07 05:27:52,966 >> storing https://huggingface.co/distilgpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1309] 2021-03-07 05:27:52,967 >> creating metadata file for /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1302] 2021-03-07 05:27:53,525 >> https://huggingface.co/distilgpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptu37oq2f\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 513kB/s]\n",
            "[INFO|file_utils.py:1306] 2021-03-07 05:27:54,970 >> storing https://huggingface.co/distilgpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1309] 2021-03-07 05:27:54,970 >> creating metadata file for /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1302] 2021-03-07 05:27:55,532 >> https://huggingface.co/distilgpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm2jji81s\n",
            "Downloading: 100% 1.36M/1.36M [00:01<00:00, 1.26MB/s]\n",
            "[INFO|file_utils.py:1306] 2021-03-07 05:27:57,435 >> storing https://huggingface.co/distilgpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1309] 2021-03-07 05:27:57,435 >> creating metadata file for /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-03-07 05:27:57,436 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-03-07 05:27:57,436 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-03-07 05:27:57,436 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1302] 2021-03-07 05:27:58,106 >> https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpegtph33n\n",
            "Downloading: 100% 353M/353M [00:06<00:00, 58.5MB/s]\n",
            "[INFO|file_utils.py:1306] 2021-03-07 05:28:04,487 >> storing https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|file_utils.py:1309] 2021-03-07 05:28:04,488 >> creating metadata file for /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1027] 2021-03-07 05:28:04,488 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1143] 2021-03-07 05:28:08,544 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-03-07 05:28:08,544 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 40/40 [00:01<00:00, 38.82ba/s]\n",
            "100% 40/40 [00:01<00:00, 27.69ba/s]\n",
            "[INFO|trainer.py:432] 2021-03-07 05:28:11,079 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:837] 2021-03-07 05:28:11,082 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-03-07 05:28:11,083 >>   Num examples = 275\n",
            "[INFO|trainer.py:839] 2021-03-07 05:28:11,083 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:840] 2021-03-07 05:28:11,083 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:841] 2021-03-07 05:28:11,083 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:842] 2021-03-07 05:28:11,083 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-03-07 05:28:11,083 >>   Total optimization steps = 414\n",
            "100% 414/414 [2:46:27<00:00, 21.88s/it][INFO|trainer.py:1007] 2021-03-07 08:14:38,879 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 9987.8149, 'train_samples_per_second': 0.041, 'epoch': 3.0}\n",
            "100% 414/414 [2:46:27<00:00, 24.13s/it]\n",
            "[INFO|trainer.py:1408] 2021-03-07 08:14:39,074 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:304] 2021-03-07 08:14:39,095 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-03-07 08:14:40,409 >> Model weights saved in output/pytorch_model.bin\n",
            "Traceback (most recent call last):\n",
            "  File \"run_clm.py\", line 406, in <module>\n",
            "    main()\n",
            "  File \"run_clm.py\", line 380, in main\n",
            "    trainer.log_metrics(\"train\", metrics)\n",
            "AttributeError: 'Trainer' object has no attribute 'log_metrics'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNs7U8mprzsF"
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        " \n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/output')\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/output', pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoZZX5qdr58K"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode('[BOS] The King must leave the throne now. [EOS]',\n",
        "                      return_tensors='pt')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXh6YFfQR-hs"
      },
      "source": [
        "# Greedy Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHn9Uf-sqbS5",
        "outputId": "2c70b987-fc90-4d82-e151-3633139643ac"
      },
      "source": [
        "# generate text until the output length (which includes the context length) reaches 300\n",
        "greedy_output = model.generate(input_ids, max_length=300)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[BOS] The King must leave the throne now. [EOS] I'll not be so. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be gone. I'll be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8zBfqDpB61s"
      },
      "source": [
        "Text generated by greedy search repeate a lot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuMB8Q-rSr4y"
      },
      "source": [
        "# Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jrw50kIqgWO",
        "outputId": "ff9f1c46-85a6-4d6f-8a00-c0f6f2c502df"
      },
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=300, \n",
        "    num_beams=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[BOS] The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now. [EOS]The King must leave the throne now.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrFlSAT3CZL6"
      },
      "source": [
        "Text generated by beam search also repeate a lot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFZQO3cVsaSL",
        "outputId": "f2e142e5-6479-4567-a5c8-bc34886ec33b"
      },
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=300, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[BOS] The King must leave the throne now. [EOS] Come, my lord, I'll leave you to the king's side.BUCKINGHAM:I'll go, sir.KING RICHARD II:Sir, let's go.BRUTUS HENRY VI:What's the King's name?ROMEO:The King of England, the Duke of York, is the son of Richard III, and the heir of the crown.The king of France, Richard II, was born in France in the year of Henry VI.He is known to be a traitor to France and to his country, as well as an enemy of his own.First Lady Margaret of Gloucester, daughter of Edward IV, died in Paris in December of this year.Second Lady Anne of Warwick, son-in-law of Prince Edward III and his son, Edward VI, were both executed by the French army on the night of December 11th, but were not executed on that night.Third Lady Edward II died at the age of twenty-two years old.Fourth Lady Elizabeth II was a prisoner of war for the purpose of her life.She was executed for treason by her husband, Henry IV.Fifth Lady Henry III died on December 12th.Sixth Lord Edward I was hanged by his wife, Margaret, in York's Tower of London.Seventh Earl of Lancaster, who had been executed in London for his treason, did not die on his death\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lLs2U6zCpET"
      },
      "source": [
        "Adding no_repeat_ngram_size to decode can force model not generate repeating text, but results are not so great."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGk9hlyDTtnR",
        "outputId": "ccd24eb5-cb4d-4468-ca5f-e5b74c5d3ccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=300, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=3, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# now we have 3 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: [BOS] The King must leave the throne now. [EOS] Come, my lord, I'll leave you to the king's side.BUCKINGHAM:I'll go, sir.KING RICHARD II:Sir, let's go.BRUTUS HENRY VI:What's the King's name?ROMEO:The King of England, the Duke of York, is the son of Richard III, and the heir of the crown.The king of France, Richard II, was born in France in the year of Henry VI.He is known to be a traitor to France and to his country, as well as an enemy of his own.First Lady Margaret of Gloucester, daughter of Edward IV, died in Paris in December of this year.Second Lady Anne of Warwick, son-in-law of Prince Edward III and his son, Edward VI, were both executed by the French army on the night of December 11th, but were not executed on that night.Third Lady Edward II died at the age of twenty-two years old.Fourth Lady Elizabeth II was a prisoner of war for the purpose of her life.She was executed for treason by her husband, Henry IV.Fifth Lady Henry III died on December 12th.Sixth Lord Edward I was hanged by his wife, Margaret, in York's Tower of London.Seventh Earl of Lancaster, who had been executed in London for his treason, did not die on his death\n",
            "1: [BOS] The King must leave the throne now. [EOS] Come, my lord, I'll leave you to the king's side.BUCKINGHAM:I'll go, sir.KING RICHARD II:Sir, let's go.BRUTUS HENRY VI:What's the King's name?ROMEO:The King of England, the Duke of York, is the son of Richard III, and the heir of the crown.The king of France, Richard II, was born in France in the year of Henry VI.He is known to be a traitor to France and to his country, as well as an enemy of his own.First Lady Margaret of Gloucester, daughter of Edward IV, died in Paris in December of this year.Second Lady Anne of Warwick, son-in-law of Prince Edward III and his son, Edward VI, were both executed by the French army on the night of December 11th, but were not executed on that night.Third Lady Edward II died at the age of twenty-two years old.Fourth Lady Elizabeth II was a prisoner of war for the purpose of her life.She was executed for treason by her husband, Henry IV.Fifth Lady Henry III died on December 12th.Sixth Lord Edward I was hanged by his wife, Margaret, in York's Tower of London.Seventh Earl of Lancaster, who had been executed in London for his treason, did not die.DU\n",
            "2: [BOS] The King must leave the throne now. [EOS] Come, my lord, I'll leave you to the king's side.BUCKINGHAM:I'll go, sir.KING RICHARD II:Sir, let's go.BRUTUS HENRY VI:What's the King's name?ROMEO:The King of England, the Duke of York, is the son of Richard III, and the heir of the crown.The king of France, Richard II, was born in France in the year of Henry VI.He is known to be a traitor to France and to his country, as well as an enemy of his own.First Lady Margaret of Gloucester, daughter of Edward IV, died in Paris in December of this year.Second Lady Anne of Warwick, son-in-law of Prince Edward III and his son, Edward VI, were both executed by the French army on the night of December 11th, but were not executed on that night.Third Lady Edward II died at the age of twenty-two years old.Fourth Lady Elizabeth II was a prisoner of war for the purpose of her life.She was executed for treason by her husband, Henry IV.Fifth Lady Henry III died on December 12th.Sixth Lord Edward I was hanged by his wife, Margaret, in York's Tower of London.Seventh Earl of Lancaster, who had been executed in London for his treason, did not die on this night\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmZI0SZ-DosE"
      },
      "source": [
        "Beam search can return top results(less than num_beams), so we can manually pick the best one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRZu5TaCUF87"
      },
      "source": [
        "# Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjeMJN_qUCJy",
        "outputId": "2d22912d-4131-47ca-9de6-13df8552fdf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[BOS] The King must leave the throne now. [EOS] Mere invention for thy messengerator.KING OF HENRY VI:There is no preparation.DECUTIO:Who know you now? KING OF HENRY VI:I have his royal catchall and make power official.DECUTIO:My lord, know you would obey the king.KING OF HENRY VI:But make our king's watch.DECUTIO:Servant, what you shall hear?KING OF AUDOLA:The king is not, sir.DAVID:My lord, I do know your prince Tuke,regarde who holds the crown and, from whom he is spoken.KING OF HENRY VI:Who is Lord Duke Tuke? Duke Tuke, name your prince, Duke Tuke, both King Duke Tuke and Duke Kale.ABULT:Father Tuke, you must now see that duty.KING OF HENRY VI:For the king, I call upon you, of my godly purism to speak your mind.ABULT:I'll take the command we need to assist thee. I need to come to your ear, and speak certain words of your tongue.DECUTIO:Ay my lords, speak those words, Duke Fathipokaris, which the king begins to speak.ABULT:What we need, do us our best. WHEREARD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpupozHFmhv"
      },
      "source": [
        "Rather than pick the highest probability words, sampling method randomly pick next words. In this way, text generated is more flexible, but sometimes doesn't make sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAWGLsqbUVY7",
        "outputId": "6e094ed5-9705-4655-f202-5d9fdb3c0b7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[BOS] The King must leave the throne now. [EOS] Merely for thy messenger, and shall not be found;Nor shall he be found. KING RICHARD III:Who shall be found? KING RICHARD III:I think he shall be found. KING RICHARD III:Why, King?KING RICHARD III:My cause, I am on my way. KING RICHARD III:My cause, I am all well. KING RICHARD III:Where should I find my way?Where should I find my way?Where should I find my way?Where should I find my way?Where should I find my way?Where should I find my way?Where should I find my way?Where should I find my way?Pray, kindly king, and welcome you.KING RICHARD III:Haggard, how come I stay here?BOS:I have heard you speak of thy messenger, and he hath not yet been found.KING RICHARD III:Let's go to London.KING RICHARD III:Let's go.KING RICHARD III:Where shall I find my way?BOS:I have heard you speak of thy messenger.KING RICHARD III:I have heard you speak of thy messenger.KING RICHARD III:How come you speak of the king?BOS:I have heard you speak of thy messenger.KING RICHARD III:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30mEePymGiLL"
      },
      "source": [
        "Temperature feature can sharpen words ditribution, avoid picking low probability words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xph7GD-vUtSw"
      },
      "source": [
        "# Top-K Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FFI1khoUlzA",
        "outputId": "c88dbdad-66ba-4689-9a26-33d25f86aa63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[BOS] The King must leave the throne now. [EOS]Tis nothing for thy will, and shall not change my view.Nurse, stay with you.CUTUS:As a soldier, but one for whom you, 'tis a soldier and a prophet.SICINIUS:Why, it is a traitor of justice? O, the king's blood on my head.Nurse:Well, 'tis an exile, but one where he is known for fighting.CUTUS:O my oath, sir.ISABELLA:Is the crown a sword, sir?ISABELLA:The crown is a dagger, sir?ISABELLA:No, sir.Nurse:The crown is a dagger that comes with a sword.ISABELLA:Why, it is a traitor to a King.A:Why, the king's blood.ISABELLA:A royal dagger, sir.ISABELLA:So, sir, why the king's blood?ISABELLA:Was the king in distress?ISABELLA:But, no, sir, 'tis false.Is it a royal sword to the king? ISABELLA:No, sir, it is a sword, Sir.ISABELLA:And, 'tis true.ISABELLA:Ay, but I believe you have your brother Edward VI.ISABELLA:To you.IS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jesCaYAT2swW"
      },
      "source": [
        "Top-K sampling pick the k highest probability words, and redistribute probabilities among them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNNRzbl3VQEK"
      },
      "source": [
        "# Top-p (nucleus) Sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhukMAqDVFX2",
        "outputId": "f12f2a68-297c-4461-e431-8594a8729c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[BOS] The King must leave the throne now. [EOS] Mere invention for thy own good.KING OF HENRY VI:There is no preparation.DEALINGS:Here it is, King Edward, for this power, amending it to his royal masters.First Son:My lord, are you deposed in this matter? EDWARD IV:Farewell, my lord.First Son:Now our king needs an absolute successor.First Son:Doubt not.First Son:And none worse. EDWARD IV:The king is not, my lord, my lord, any more.First Son:Doubt, my lord.Second Son:Take, my lord; consent your will.Second Son:Sir, Richard.First Son:Confess this, my lord, that I have dawd you to a King.A noble lord.KING OF HENRY VI:To your purposes, you must now have a duty.First Son:Sir, pray not.First Son:What? my lord, of my lord?First Son:To your mind, as I did, who gave my hand, we shall see the power of royal succession to come to your end.KING OF HENRY VI:And to our nobles, how are you?Second Son:Very well, but you are made.Second Son:Ay, but I am made a king.KING OF HENRY VI:Bass you.Second\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IGMYqzd5aKS"
      },
      "source": [
        "Top-p Sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdOzPn0SVj55",
        "outputId": "05befd2e-3166-4e16-b758-9767f460635f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: [BOS] The King must leave the throne now. [EOS]Tis very strange; my father shall not be king.I'll not leave him, if he should.JULIET:I think I should leave him to him: he should stay.WARWICK:What then?EOS:He, you have lost him.WARWICK:Is it true; let's leave me!WARWICK:Is it true?EOS:He'll leave him to him, if he should, which is true!JULIET:Nay! I'll leave him.WARWICK:I'll leave him to the king: I will leave him to him.WARWICK:I am a man to be king, but he shall stay--WARWICK:Yesternock, I shall leave him to me and to the king's son, and the crown shall stay--WARWICK:He shall stay to the king, and not stay: I will stay his and to the king's son;And the crown shall remain.WARWICK:That's very strange; your father cannot stay, you know, my father.JULIET:Nay!Your father is gone;And his sons are gone.WARWICK:He must be gone; and the king shall not stay.WARWICK:Why? You have been slain.WARWICK:O, then, come on up the highway\n",
            "1: [BOS] The King must leave the throne now. [EOS] I have an assurance, and I trust it well: I think he has made his name as a traitor to this man.KINGSLEY:Where lies that the king's son is in here?KING JESUS HENRY:He is a traitor to the sovereign. KING JESUS HENRY:He lives to see what it is that the King hath done; that he is to be seen and heard as a traitor.KINGSLEY:To the king himself he should have been hanged; that he could never go to prison.KINGSLEY:The king's soul shall be made in Rome, he shall be free by death.KING JESUS HENRY:So here it is in his hands that I swear, as the oath of obedience is.KING JESUS HENRY:He should have been hanged.KING JESUS HENRY:My oath, that I swear, is not made, not in the realm of Rome.KING JESUS HENRY:But my oath, that I swear, is not made.KINGSLEY:My oath, that I swear, is not made.KING JESUS HENRY:No, my oath, that I swear.KING JESUS HENRY:That the king should have fled to Rome.KING JESUS HENRY:That I do not swear.KING JES\n",
            "2: [BOS] The King must leave the throne now. [EOS] 'I shall not, 'Tis a sad day, and a stormful storm:'My Lord 'tis 't the worst,' 'tis my wife and my son, and my son-in-law,'And for that night we are on the march to the grave,'I am not sure where the king's tomb lies,Or where the tomb,Or where there is no burial place;What the hell is a coffin? I have the answer in my mind,And yet I cannot see what's there;In the morrow, with so much blood and so much blood,And to see that the coffin stands, is in heaven's midst. I pray him the same,And for the grave, and all of me, so long that he lies buried,And my love shall be for God to bury with him. O King! 'tis not a sad day, and a stormful storm.BENUMERIC IUDERIC II:And what in his absence is not his absence? I do not know what. O King! what's 'tis not? O King! 'tis not.CATHOLIC IV:In his absence is his absence; I cannot perceive his absence. O King! 'tis not.Marry 'tis not.CATHOLIC IV:I should not say so, though I will tell you what he did not 'tis,If\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Vbx6tvAxdu"
      },
      "source": [
        "Combine top-k and top-p together, result is much better, it does look like Shakespeare style, altough take a closer look, you'll find it's not written by Shakespeare. Anyway, it's kind of amazing that language model can produce such good text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1HJrQ_XzAqR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ0Hpbil3gM8"
      },
      "source": [
        "**References**\n",
        "\n",
        "[How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)\n"
      ]
    }
  ]
}